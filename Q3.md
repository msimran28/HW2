Q3 Online news
================

### import the model online\_news

# look at the data and examine it.

### first “regression and threshold”

# note that shares is hugely skewed

# probably want a log transformation here

![](exercise_3_files/figure-gfm/unnamed-chunk-1-1.png)<!-- -->

# much nicer :-)

![](exercise_3_files/figure-gfm/unnamed-chunk-2-1.png)<!-- -->

#### lasso (glmnet does L1-L2, gamlr does L0-L1)

# I want to fit a lasso regression and do cross validation of K=10 folds

# inorder to automate finiding independent variables and training & testing my data multiple times.

# cv.gamlr command in the gamlr does it for me.

# download gamlr library

# i create a matrix of all my independent varaibles except for url from online\_news data to make it easily readable for gamlr commands.

# the sparse.model.matrix function.

    ## Loading required package: Matrix

    ## Warning: package 'Matrix' was built under R version 3.6.3

# Here I fit my lasso regression to the data and do my cross validation of k=10 n folds

# the cv.gamlr command does both things at once.

\#(verb just prints progress)

    ## fold 1,2,3,4,5,6,7,8,9,10,done.

# plot the out-of-sample deviance as a function of log lambda

![](exercise_3_files/figure-gfm/unnamed-chunk-5-1.png)<!-- -->

## CV min deviance selection

    ## [1] -6.563407

    ## [1] 33

# predict number of shares

    ## 50 x 1 Matrix of class "dgeMatrix"
    ##       seg42
    ## 1  1408.006
    ## 2  1542.736
    ## 3  1646.186
    ## 4  1457.618
    ## 5  2093.053
    ## 6  1895.144
    ## 7  2133.779
    ## 8  2211.473
    ## 9  1756.224
    ## 10 1244.756
    ## 11 1359.895
    ## 12 1835.206
    ## 13 2246.174
    ## 14 2000.967
    ## 15 2059.799
    ## 16 1340.426
    ## 17 2104.288
    ## 18 1603.093
    ## 19 1855.942
    ## 20 2302.021
    ## 21 2128.996
    ## 22 1273.504
    ## 23 2076.488
    ## 24 1303.985
    ## 25 1670.990
    ## 26 1938.403
    ## 27 1846.150
    ## 28 2266.455
    ## 29 1823.412
    ## 30 1745.557
    ## 31 1671.994
    ## 32 1893.875
    ## 33 1909.067
    ## 34 1963.917
    ## 35 1868.949
    ## 36 1957.720
    ## 37 2286.984
    ## 38 2130.814
    ## 39 1295.405
    ## 40 1391.076
    ## 41 2109.136
    ## 42 1741.179
    ## 43 2266.250
    ## 44 2148.708
    ## 45 2083.287
    ## 46 1606.383
    ## 47 2142.245
    ## 48 1518.487
    ## 49 2113.150
    ## 50 2088.287

# change predicted number of shares into viral prediction(t\_viral)

    ##  [1] 1 1 1 1 1 1 1 1 1 0 0 1 1 1 1 0 1 1 1 1 1 0 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1
    ## [39] 0 0 1 1 1 1 1 1 1 1 1 1

# create new variable “viral”

    ##  [1] 0 0 1 0 0 0 0 0 1 0 1 1 0 1 0 1 1 1 1 1

# create confusion matrix

    ##    yhat
    ## y       0     1
    ##   0  4534 15548
    ##   1  1891 17671

    ## [1] 0.56011

##### model 2

# create logistic lasso regression and cross validate with viral as the dependent variable

# add family = “binomial” to code to do a logistic regression instead of normal regression

\#(verb just prints progress)

    ## fold 1,2,3,4,5,6,7,8,9,10,done.

# plot the out-of-sample deviance as a function of log lambda

![](exercise_3_files/figure-gfm/unnamed-chunk-12-1.png)<!-- -->

## CV min deviance selection

    ## [1] -7.190693

    ## [1] 32

# predict number of viral

    ## 50 x 1 Matrix of class "dgeMatrix"
    ##           seg57
    ## 1  -0.801157615
    ## 2  -0.301428954
    ## 3  -0.198991462
    ## 4  -0.636498179
    ## 5   0.290362197
    ## 6   0.263415715
    ## 7   0.236272187
    ## 8   0.441440387
    ## 9   0.005897933
    ## 10 -0.925096220
    ## 11 -0.631327582
    ## 12  0.002989255
    ## 13  0.218076658
    ## 14 -0.068471046
    ## 15  0.280117787
    ## 16 -0.727856910
    ## 17  0.119734044
    ## 18 -0.154424893
    ## 19 -0.086025209
    ## 20  0.413414364
    ## 21  0.364412981
    ## 22 -0.808823390
    ## 23  0.235908351
    ## 24 -0.737822693
    ## 25 -0.297959192
    ## 26  0.314264802
    ## 27  0.170029560
    ## 28  0.465537368
    ## 29  0.068823180
    ## 30  0.048938032
    ## 31 -0.089716572
    ## 32  0.158023342
    ## 33  0.236833096
    ## 34  0.242897570
    ## 35  0.049713407
    ## 36  0.195914112
    ## 37  0.393725984
    ## 38  0.623988950
    ## 39 -0.832409594
    ## 40 -0.718406381
    ## 41  0.300886279
    ## 42  0.070514747
    ## 43  0.341878834
    ## 44  0.389195606
    ## 45  0.256635987
    ## 46 -0.278208781
    ## 47  0.189654340
    ## 48 -0.525570469
    ## 49  0.302603313
    ## 50  0.262613471

# change hat\_viral to true/false prediction

    ##  [1] 0 0 0 0 1 1 1 1 1 0 0 1 1 0 1 0 1 0 0 1 1 0 1 0 0 1 1 1 1 1 0 1 1 1 1 1 1 1
    ## [39] 0 0 1 1 1 1 1 0 1 0 1 1

# create confusion matrix

    ##    yhat
    ## y       0     1
    ##   0 12208  7874
    ##   1  6796 12766

    ## [1] 0.6299566

#### comaprison of models

    ## viral
    ##     0     1 
    ## FALSE FALSE

    ##    yhat
    ## y       0     1
    ##   0  4534 15548
    ##   1  1891 17671

    ## [1] 0.56011

    ## [1] 0.7753597

    ## [1] 0.7481327

    ## [1] 0.4625331

    ##    yhat
    ## y       0     1
    ##   0 12208  7874
    ##   1  6796 12766

    ## [1] 0.6299566

    ## [1] 0.6494223

    ## [1] 0.3889553

    ## [1] 0.3807272

# Summary: The actual percentage of not viral in the data set is 50.66% Hence, there is 50 percent chance the article be viral or not viral. So, our null hypothesis is ‘not viral’. Model 1 predicts an article would not 56.24% times correctly and model 2 predicts 63% times. Thus, model 1 is about 6% times better and model 2 is about 13% times better than null hypothesis. The true positive rate for model 1 is 77.5% and for model 2 it is 64.9%. Here, model 1 does a better job. The false positive rate for model 1 is 74.8% while for model 2 it is 38.9%. Here, model 2 is better because lower FPR in general is better. Similary, false discovery rate for model 1 is 46.3% and for model 2 its 38%. Again, model 2 is better because lower FDR is better for prediction. Therefore.,based TPR, FPR, FDR, and general acuracy overall model 2 does better than model 1.
